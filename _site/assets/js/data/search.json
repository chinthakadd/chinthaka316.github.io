[
  
  {
    "title": "Contract Testing for Event Driven Architectures - Can We, Do We and How Do We?",
    "url": "/posts/Contract-Testing-For-EDA/",
    "categories": "",
    "tags": "",
    "date": "2022-02-24 00:00:00 +0000",
    





    "snippet": "Contract Testing for Event Driven Architectures: Can We, Do We and How Do We?In recent past, I had the chance to involve deeply in setting up a developer testing strategy for microservices that follow Event Driven Architecture (EDA) patterns. As a part of this, we performed a deep dive analysis to understand how to introduce Contract Testing to EDA.While Contract Testing is an important testing concept which became extremely popular in the microservices world, it is mostly associated with the RESTful realm. Therefore when it comes to messaging, we wanted to evaluate how contract testing fits in the test pyramid. Can we actually perform Contract Testing? Are there any challenges? What value does it bring? Pressed reset button and we went into research mode as it required us to dig deeper.In this article I would like share what we learnt and the opinion that we built around it. As all opinions go, it is subjected to change, but given the current context, we think it is the right way to go.First of all, why are we even talking about Contract Testing?I am not going to bore you with my explanations around how Contract Testing typically work. Instead I have referenced some of the good literature around that in References section.At a gist, Contract Testing is a verification mechanism to ensure that producer - consumer relationships that get formed due to API based integration are maintained in a healthy fashion. Given it is a Test that lives lowers in the test pyramid, it allows you to identify such potential breaking changes earlier in the development lifecycle. In essence, Contract Testing is all about testing the honesty of consumers and producers and evaluate how truthful they are to each other in this relationship they have built. It provides a platform for communication and collaboration and helps them built a long lasting relationships. If you want to learn Consumer Driven Contracts and also have a good laugh while doing so, I recommend watching the following presentation by Ben Sayers and Mauri Edo from 2016 VideoContract Testing is a well established pattern in the REST world. Specifically the Consumer Driven Contract (CDC) Testing is widely adopted. In CDC, Consumer writes the contracts that defines their expectations and publishes it. These contracts are verified against the Producer APIs. There are two main frameworks for Consumer Driven Contract Testing that are widely adopted.Those are:      Pact - https://docs.pact.io/        Spring Cloud Contract - https://cloud.spring.io/spring-cloud-contract/reference/html/project-features.html  While the workflows provided by Pact and Spring Cloud Contract (SCC) have subtle differences, the concepts stands similar and can be applied nicely in the REST scenario.Apart from Consumer Driven form of it, Contract Testing has grown in scope with introduction of various sub concepts such as Provider Driven Contract Testing, Bi-Directional Contract Testing and Schema Driven Contract Testing and etc.On the other hand, Event Driven Architecture (EDA) and Messaging has a very very broad scope as well. I am not going to get into those details. Instead let me get back to what our original intention was and define the scope our EDA that is more specific and relatable to most industry standard implementations.Assume the following:  We have an microservice architecture which uses Kafka as the messaging platform.  These microservices are Event Driven and uses two main patterns: Events and Command-Reply. There are two forms of Producer - Consumer relationships.          Producer microservices raises events of interest to a Kafka Topic and Consumer microservices that are interested in those events will listen to that Kafka topic      Consumer microservices raises commands via a Kafka Topic and Producer microservices act on those commands and reply with a Reply Kafka Topic        Kafka Messaging uses Avro as the Serialization mechanism (Or assume other schema driven serialization formats but definitely not JSON)  We also have a Schema Registry in place to support schema collaboration and compatibility verification (assume Confluent Schema Registry)TODO: Simple Diagram Explaining the EDA described above (MS, Kafka, Schema Registry)If we are thinking of an Event Driven Architecture using Kafka as the Messaging Platform, this would be a common scenario. Next we need to look at how Contract Testing becomes applicable in this context…Can Contract Testing be applicable for Messaging?Well, given Messaging is also a form of API, Contract Testing can apply to messaging and EDA as it also results in Producer - Consumer relationships very similar to REST.So let’s try to define the requirement for Contract Testing in the messaging use-case. Let’s start with Consumer Driven Contract Testing for event messaging and define our requirement.Given:I have a suite of Event Driven Microservices that communicate with each other using Event MessagesWhen:A Producer - Consumer relationship is formed when a Consumer subscribed to Producer&#39;s Event TopicThen:I want the Consumer to define their expectations in the form of a ContractAnd:I want the Producer to comply with this contract and run verification tests against the Producer&#39;s Event Publication logic before it gets deployed to productionThis looks a very reasonable requirement. Now how do we achieve this? To find answers to this question,  we needed to look at Pact and Spring Cloud Contract and see how can use those frameworks to support this requirement.  How Spring Cloud Contract provides messaging support?https://cloud.spring.io/spring-cloud-contract/reference/html/project-features.html#features-messaging  How Pact provides messaging support?https://docs.pact.io/getting_started/how_pact_works#how-to-write-message-pact-testsTODO: Diagrams explaining the difference between the two approaches.Diving deep into these two approaches, we came to the realization that these frameworks have very different ideologies when it comes to messaging support of contract testing (with different but valid reasons of course)As I mentioned before, in the REST APIs, both Pact and SCC follow the same principles. Both frameworks have implemented Consumer Driven Contract Testing as a form of Integration Testing.When I mean Integration Testing, I really mean it because it involves a wire protocol. On consumer side, the actual client layer is tested against a HTTP mock server. On producer side, the actual REST / HTTP API is invoked and response is validated against the consumer contract.But when it comes to Messaging…Spring Cloud Contract follows a consistent approach here as well and Contract Testing happens against the actual Broker like a typical integration test. But Pact instead recommends that we test the service layer not the real integration layer. More like a unit test.&quot;&quot;&quot;We recommend that you split the code that is responsible for handling the protocol specific things - for example an AWS lambda handler and the AWS SNS input body - and the piece of code that actually handles the payload.&quot;&quot;&quot;Quoted from: https://docs.pact.io/getting_started/how_pact_works#how-to-write-message-pact-testsIf we follow the Pact Approach, it means that Contract Testing will follow different philosophies for messaging vs. REST.So we started asking ourselves, why should we make two different choices for two different protocols.For Pact as a Framework, their design choice makes sense to them.To reiterate: Pact does not know about the various message queueing technologies - there are simply too many! And more importantly, Pact is really about testing the messages that pass between them, you can still write your standard functional tests using other frameworks designed for such things.Quoted from: https://docs.pact.io/getting_started/how_pact_works#how-to-write-message-pact-testsBut would that simply makes sense for us when we have standardized our architecture on one messaging protocol and that is Kafka? Therefore even though it is an approach that would definitely work, we were reluctant to subscribe to the idea of testing contracts in Service Layer instead.SCC on the other hand allows us to be consistent. So the use of SCC approach for messaging started to be bit more interesting and aligned with our thought process.But more to come…When we talk about REST, we implicitly mean REST / JSON most of the time. But in messaging, JSON is not the standard. Remember the scope I outlined as well. We want to use a schema oriented serialization format, i.e. Avro.Does SCC support Kafka / Avro? In fact, does SCC support Kafka / Avro + Schema Registry? Because use of Avro in Kafka gets augmented a bit with the use of Schema Registry. The answer is No. There is no direct support for Avro or Schema Registry.This is where we came to a bit of dilemma. How do we go from here? So we started breaking the problem down. Let’s focus on actual difference of REST vs. Messaging with Kafka.In REST APIs, we define a Specification using Open API. The term “specification” is the key here. It is not a real schema. We use JSON as the serialization format for request and responses which is not schema oriented. The lack of such schema orientation made Contract Testing absolutely vital in the context of REST. We had no other choice to validate if the specification is actually implemented in the producer side.Also most producer APIs were often poorly designed by our own mistakes breaking single responsibility principle. We often made APIs do many things in one, and it was easy because there was no schema or a rule that prevented us from doing so. If the design was proper, the specification itself could have covered most of the contracts. But it never did.  On the other hand, specifications are not capable to upheld compatibility rules. They were not meant for it.Can we learn from REST and do somethings different in Kafka?Yes we can. In Kafka with Avro, we have one more additional tool at hand. That is the Schema. Schema is powerful than the specification in this respect. You can apply compatibility rules for a schema and make sure that producers can only change their production within a limited scope that doesn’t break consumers. Schema Registry provides this verification capability and it can be done far early in the development cycle even before testing.Next let’s think about message modeling. In an Event Driven Architecture, messages in the form of Events and Commands are entitled to have a Strong Schema definition. Those must be defined with single responsibility in mind. For instance, we should not create a generic multi-purposed command messages grouping many commands. That easily creates confusions and many interpretations of the same schema introducing the need for different consumer contracts. (TODO: Good example goes here). Instead a message model should be well defined with Single Responsibility rule in consideration.So if our architecture is based on the following:  All message models and topics are defined with Single Responsibility rule in mind.  We define such strong schema definitions with only small percentage of relaxed rules for attributes that are optional  We apply proper compatibility rules for all schemas and verify them through schema registryIf we can upheld all of the above, is it still worth doing Consumer Contract Testing for messaging? This was a question that we gave serious serious thought into.Further more and finally. Contract Testing is most effective when you have a manageable number of consumer-producer relationships. Specifically Consumer Driven Contracts are most efficient when consumers are well-known and are less in number. But in the messaging and EDA, this could not be even practical. Events specifically are supposed to be open for consumption and subscribed by any other microservice interested in that event. So EDAs can easily lead to (Too) Many Consumers to One Producer relationships that makes things difficult to manage Consumer Driven Contract.So it becomes a trade-off between the value of testing vs. the maintainability cost of these tests.Is CDC the only Option here for Messaging?This was a question that we asked the community as well.https://stackoverflow.com/questions/71028845/do-we-need-to-do-consumer-driven-contract-testing-kafka-driven-micro-services-whAn alternative would be to write Producer Driven Contract Tests instead. However at present, the same challenges presented above for CDC in messaging applies in here as well. But this is an approach that may be promising if we can have tooling for it.Therefore, given the current context, looking at the messaging and schema oriented serialization format support of Contract Testing tools, our team decided that this is not the right time to invest our efforts in writing Contract Testing for Messaging.Instead, we focus our energy on defining single responsibility driven messaging schemas, binding schema compatibility rules for our topics and messages and finally doing Schema Compatibility Testing leveraging the Schema Registry.Like I said, we will keep an open mind about this. The community is very active in this area and a good amount of discussions are happening around the future of Contract Testing. So we hope we have better answers as we keep digging. But at this point, I wanted to share these thoughts hoping that it will benefit others who are going through the same research. I am sure this is a recurring problem.Of course if there are better alternatives, feel free to suggest and always happy to learn!!! Cheers..!!References:TODO:…"
  },
  
  {
    "title": "Learning Sagas Step by Step",
    "url": "/posts/Learning-Sagas-Step-By-Step/",
    "categories": "",
    "tags": "",
    "date": "2021-11-27 00:00:00 +0000",
    





    "snippet": "Saga Pattern attempts to solve one of the biggest challenges encountered in micro-service architecture in terms of managing transactions across service boundaries. However implementing Sagas is a challenge on its own for us developers (as opposed to ACID transactions) and requires lot more knowledge, experience and research. I am writing this article to share the knowledge I have gathered through the journey of my research and also share my own thoughts and interpretations through this article.Role of a Transaction and How Does It Apply to Distributed System ArchitectureThe concept of transaction is extremely important in achieving data consistency. Transactions in the form ACID (Atomicity, Consistency, Isolation and Durability) are the best perceived form of transactions that developers are used to deal with. ACID transactions provide definitive means on how to manage data while a transaction is in play and preserves the ACID properties.  When the data is local to a single system backed by may be one or few data stores, ACID can be implemented pretty seamlessly. In Java and Spring World, we are so used to the Transactional Specification of Java EE and Awesome AOP based Spring Implementation of Transactions (@Transactional).While ACID is great, achieving all these 4 properties at once becomes complicated in the context of distributed systems. The term Distributed Transactions sounds to be the solution for this. Essentially the idea is to have a Distributed Transaction Manager (Coordinator) that can coordinate transactions across multiple systems and ensure that transaction is performed only with consensus of all participants. This orchestration is not that simple and not all that efficient when it comes to the distributed world. It gets even further complicated when participating systems operate asynchronously and in a event driven fashion.Distributed Transactions - Why Not? What are the Challenges?The defacto standard for Distributed Transactions is Open XA (Extended Architecture) which uses the concept of Two Phase Commit. For XA Transactions to be implemented, it requires that the participating data stores are inherently XA Compliant. Most SQL databases (ex: Oracle) and legacy message brokers (ex: IBM MQ) are XA compliant. If we are building our distributed systems in an ecosystem which is fully XA compliant, implementing Distributed Transactions may truly be possible. There are frameworks such as Atomikos and Narayana which provides Distributed Transaction Management implementations as well. But…Challenge 01:  Modern databases and messaging systems  are not  XA compliantWhen building distributed systems in modern world, you would want to use different data stores and message brokers based on the the need. You want to use a NoSQL Database like Mongo, Cassandra or Neo4J or use a messaging system like Kafka or RabbitMQ for that matter. None of these would support XA Transactions which means you cannot apply Distributed Transaction solution when these data stores and messaging systems are involved.Challenge 02: You can ONLY choose Consistency Over Availability with XA Transactions in placeWhat is meant by that? If you apply CAP Theorem to a Distributed System, we know that Partition Tolerance is a given, which means we need to trade-off between Consistency and Availability and pick one, because achieving both is not possible. In building micro-services for example, we often would want to select Availability over Consistency in many of the use cases. Some form inconsistency in the system is tolerable but not the loss of availability as it could very well mean loss of business.Now, with XA transactions, suddenly you don’t really have a choice in the matter. If a transaction is in play, for it to succeed all participant must be available at that given point in time. Even if a single participant is unavailable, the transaction would fail to commit. This means that as the number of distributed components participating in a transaction grows, the availability of that operation goes down. This is a major concern in XA transactions that simply puts the XA Transaction out of place in the Micro-service Architecture.Apart from these main two challenges, performance and scalability also becomes a factor against going XA as it requires synchronization and isolation to ensure ACIDity of operations. Also, many business functions are asynchronous and may also be long running in which case, this strategy of holding resources would not make sense.If not Distributed Transactions, then … ? Lets talk about Sagas next.What is a Saga?Sagas are essentially a sequence of local transactions that are executed in a distributed system. Each local transaction is performed by  a single service. The sequence of these local transactions must be coordinated in order to complete the overall work. That can be achieved in two ways: Using Choreography (Every Participant knows how to coordinate with other participants to get the job done) OR Using Orchestration (Central Coordinator performs the Saga).However…Sagas does not provide I in ACID transactions (Sagas by nature are ACD Only).  Isolation means that a transaction should always be performed in isolation not affecting other transactions and it should look like all events of a transaction happened in sequence. And the changes made within the transaction should not be visible to other transactions before it completes. This is not possible with Sagas because all local transactions get committed before the entire saga completes. This means that there could be anomalies that could occur in the system such as:  Loss of Updates - Due to multiple sagas concurrently executing on same data  Dirty Reads - Due to local transactions getting committed, readers may be able to read data in a state that is not overall consistent due to the Sagas that are still in progress.  Non-Repeatable Reads / Phantom Reads - Again due to concurrent execution of Sagas, the state of data of a particular row or table may change and return different results to the same query between steps.Also, unlike Transactions which provide a boolean signal to all participants to either Commit or Rollback, the same would not be possible for a Saga. In a Saga, each local transaction needs to be completed before the next begins, and so on and so forth. Which means, if something happens to a single transaction in the middle of the Saga, there is not boolean signal to rollback all that is done so far and go back to zero. Remember, all participants of a Saga are distributed and they work on their own data stores. Therefore the concept of rollback is handled differently in Sagas.Compensating TransactionsEach participants of a Saga should provide a mechanism for performing a compensating transaction that brings the system back to a consistent state when applied. This is an important aspect of the contract that a Saga Participant need to adhere to. Whenever a Saga Transaction encounters an error while being in-progress, it will start the compensation process as the means to bring the system back a consistent state.Compensating Sequence of a Saga would be in the reverse order of the normal execution of Saga and would begin from the step before the one that just failed. In S1 -&amp;gt; S2 -&amp;gt; S3 -&amp;gt; S4 sequence of Saga, if a failure happens in S4 step, Compensating Sequence of C3 -&amp;gt; C2 -&amp;gt; C1 will be executed.Do all steps of a Saga need a Compensating Transaction? Answer would be No. At times a step in a Saga would be a Read-Only operation of some sort that does not change the state of the overall system. Such steps does not require a compensating transaction. Also if a particular step can be idempotently completed with multiple retries (Retriable Transactions) or failure does not have any impact to the overall saga completion, they also do not require a compensating transaction.As discussed before, there are two ways in which Saga Pattern can be implemented, which are choreography and orchestration.  While they are self explanatory terms, lets compare how they can be implemented in a micro-service architecture to get a better understanding.  ChoreographyChoreography based Sagas are executed through message passing between micro-services. There is no central coordinator doing all the work. If you are already subscribed to the idea Domain Driven Design and Event Driven Architecture and follow the concepts such as Domain Events, the micro-services you build would already be publishing domain events and listening the other interested domain events. That foundation can be the base of a Choreographic Saga implementation as well, where micro-service work with each other in a self driven, self observatory scale to conduct Sagas. It makes things simpler in that context and remove the need of an orchestrator. Also you are able to achieve maximum loose coupling between micro-services by doing that.However, as Sagas become complex, this  style of Saga execution becomes difficult to manage and monitor. There could be many events that micro-services would have to listen to in order to make sure they perform their transactions aligning to them. It could also lead to cyclic relationship and slowly created unwanted coupling between systems if not managed properly.  OrchestrationYou will need Orchestrator to conduct the Saga Transaction. It would ensure that the step sequence is followed and initiates rollbacks as required and etc. This Saga Orchestration can be done within a micro-service itself or it can be delegated to a separate orchestration engine or workflow engine. This decision is a critical but depends on the complexity of the Saga use-cases that you require to support in your architecture. Chris Richardson in his book Microservice Patterns recommends us to use orchestration for all use-cases except for the simpler ones.Orchestration based Sagas provides more simplicity and understanding specially in case of complex Sagas. Also it’s easier to define the Saga Sequence and monitor their progress from a single point of execution. However, we should be mindful not to leak too much business logic that actually belongs to participants into the Orchestrator itself.Sagas as a State MachineA well defined Saga becomes a state machine since it has well defined set of states and state transitions.Therefore, another benefit in orchestration based approach is that sagas can be modeled as state machines and even depicted / visualized the same way. The Saga Orchestrator would always have a state and depending on forward or compensating transactions, its state would transit to one of the valid states from a deterministic set of states.The following is an example of a order processing saga depicted as a State Machine.Now, Sagas are Sequence of Local Transactions. There are 3 kinds of Transactions, and those are:  Compensatable Transactions  Pivot Transactions  Retriable TransactionsThe following diagram explain it a bit more.So the simple idea is here is the Saga can go through a set of Compensatable Transactions before reaching a Pivot Transaction. Once Pivot Transaction is reached, a decision of whether the Saga is successful or not would be taken. If successful, it will proceed further to any pending retriable transactions from that point. If any transactions post pivot point fails, they would retried until they would be made successful (since they are guaranteed to be completed) in order to complete the Saga.How to Handle Lack of IsolationThere are several ways to manage the lack the isolation and the side effects of the inconsistent reads and loss of updates.First idea that is generally discussed is to use some sort of locking mechanisms. In regular transactions, we are used to Optimistic and Pessimistic Locks and the same can be applied in the context of Saga. The idea is that a record that is currently being operated on by a Saga that is in process is not allowed to be concurrently updated through introduction of Locking. This prevents loss of updates due to concurrency of multiple saga executions (concurrent sagas updating an account balance for example). When we compare Pessimistic vs. Optimistic locking, obviously Optimistic is the way to go, given that is is far more performant compared the other. However, this would mean that Saga Participant may often encounter concurrent modification related failures while performing their part in a saga execution. It is not a real transaction failure in terms of the Saga (it is retriable) and therefore participants must attempt to retry until the local transaction is made successfully.Other forms of locks are referred to as Semantic Locks which are essentially indicators that you need to maintain within the business data tables that reflects the state of it. Such Semantic Locks can be used by the business logic to make decisions on how to proceed with any new Saga Transactions or any other requests that attempts to involve with the particular data. For example, when an order is in progress, depending on it’s status, Cancellation API can take decisions whether the Order can be allowed to be canceled or not. For example, when order is in PENDING status, it may accept it, where as when the status is moved to SHIPMENT_READY or SHIPMENT_COMPLETE, it may reject any cancellation requests. If you want to achieve full serializability of Order entity in this example, you can achieve that with a Semantic Lock. You do that by rejecting any attempts to modify the Order entity by any other Saga or other means until the current Saga is completed. Such serializability will simplify the Saga implementations, but it may not always be the case based on the business need.Similarly semantic locks can help readers be notified the record’s volatility. For example, if a Transaction is in progress, Account’s Current Balance may be volatile across multiple reads and that can be captured with an indicator flag. If the reader wants to stable current balance, it may have to repeat until the transactions complete to get a consistent value or treat the current value with the consideration of all pending transactions. For example, if customer wants to initiate a transaction while he has many other pending transactions in place, sufficient balance calculation for the account must consider all pending transactions as well.Given the concurrency and distributed nature of it, isolation is one of the toughest challenges that remain in the Saga Pattern implementation. It requires an extensive design thought process to determine how each use case will be handled with different strategies in a case by case basis.Don’t Let Compensations Over or Under CompensateIn terms of Isolation, one of the main factors to consider is how we design compensation of our own participant logic. The motive of compensation is to bring a particular part of the overall data involved in a Saga back to a consistent state. Therefore we need to be extra-cautious on how compensation logic is performed. Simplest example is how an account is credited if a particular payment is canceled. Compensation should ensure that most recent balance of the customer is credited with the exact withheld amount (Ensure no loss of updates while performing that). Sometimes compensations is simply reversing an operation already performed, but it not always the case. However, one thing for sure. We need to identify all compensatable transactions and ensure that there is a compensation logic defined for it. Otherwise it defeats the entire purpose of using Sagas to solve this problem in the first place.Finally, the concept of Sagas and its adoption in the microservices landscape is a topic that is extensively discussed in the community. Chris Richardson’s Microservices Patterns: With Examples in Java is one of the best literature for this which was the primary source of my readings as well.While there are many frameworks and products that can help build Saga Transactions for Micro-services, the two Java based libraries that comes up are Eventuate Tram (developed by Chris Richardson and his team), and Axon Framework. Other frameworks such as Eclipse Microprofile LRA, Spring Statemachine are also candidates which can support Saga development. Apart from those, we also have different workflow engine products that could be used to build Sagas.Once we understand the concepts of Sagas and determine that we most definitely need Sagas, the technology selection would be the next biggest challenge that we start facing. I am currently learning about some of these available frameworks to get a better understanding of them and hopefully I can document my learnings as I make progress.Finally I have listed few references that I have found through my research.References  Microservice Patterns by Chris Richardson - https://www.google.com/books/edition/Microservices_Patterns/QTgzEAAAQBAJ?hl=en&amp;amp;gbpv=1&amp;amp;printsec=frontcover  Saga Whitepaper: https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf  Presentation on Sagas: https://www.redhat.com/files/summit/session-assets/2019/T42224.pdf  Atomikos Distributed Transaction Manager - https://www.atomikos.com/Blog/TransactionalRESTMicroservicesWithAtomikos  Other interesting reads -          https://www.ufried.com/blog/limits_of_saga_pattern/      https://itnext.io/newscast-using-sagas-in-choreography-and-orchestration-patterns-a-java-17-and-kotlin-example-e3d0ec17b910      Frameworks:  Eventuate Tram - https://eventuate.io/abouteventuatetram.html  Axon - https://axoniq.io/product-overview/axon-server  Ecliplse LRA - https://github.com/eclipse/microprofile-lra  Spring StateMachine - https://projects.spring.io/spring-statemachine"
  },
  
  {
    "title": "Refresh Memory While Having a Deeper Look at Java equals() and hashCode() Methods",
    "url": "/posts/Java-equals-and-hashCode-methods/",
    "categories": "",
    "tags": "",
    "date": "2020-12-19 00:00:00 +0000",
    





    "snippet": "I was writing a small piece of code for a Proof of Concept, and required to write an equals method and was wondering if I had given thought to it in my career of 8 years. Yes I know it, but do I really know it enough. This was the revision exercise I did, which I thought of sharing as an simplistic article with everyone.These are two very important methods that all Java developers are well aware of. And it is such a basic topic, but may be due to the simplicity of the concept, and also due to the fact that IDEs that we use today generate them very nicely, we tend to forget or overlook it. So I thought of documenting some details around it.One of the best chapters I have read about these two methods and few others were in Chapter 03 of the book Effective Java by Joshua Bloch.In this article, I capture some of the important aspects of these two methods that I have learned.Equals - Covering BasicsLets start with Equality of Objects in Java. We know that equality of objects is evaluated using the equals method. By default, in Java, we say that two objects are equal, if the Object References (let’s say obj1 and obj2) are referring to the same object (in memory). Now we know Java Object class provides an implementation to this.public boolean equals(Object obj) {      return (this == obj);  }Pretty straight forward. But this does not suffice our needs in most cases when we exercise Object Oriented Programming.  Given that equality of instances of classes must be defined by their attributes and not based on the memory locations where they are stored, we must override this method.However, there are few rules that we need to be aware in doing this.Just the basics first. Know and understand what equality means. It is important that we ensure that all the below properties of equality are maintained by our implementation.      Reflexive - For any object reference obj, obj.equals(obj) must be true        Symmetric - For any two equal objects, obj1 and obj2, obj1.equals(obj2) and obj2.equals(obj1) both must return true.        Transitive - For obj1  obj2 and obj3 if obj1.equals(obj2) is true, and obj2.equals(obj3) is true, then obj1.equal(obj3) must be true.        Consistent - Given that attributes that are part of the equality logic remains unchanged, all subsequent equals() invocation of the same equal objects must continue to return true.  Equals - Ways to ImplementCodified Way - Every Java Developer Must KnowThe following is a hand coded implementation for equals method on Passport class./**   * Assume that passports are equal only if their number and name matches.  */ public class Passport {     private String number;   private String name;   private String country;     @Override   public boolean equals(Object o) {          // Shallow Check   		if (this == o) return true;   		// if not from same class, not equal.   		if (o == null || !o.getClass().equals(Passport.class)) return false;   		Passport passport = (Passport) o;   		// NOTE: Null safe equality check using Objects.equal().   		// If any of the value is null, it will return false,		// if both are null, it will return true. 		return Objects.equals(number, passport.number) &amp;amp;&amp;amp; Objects.equals(name, passport.name);   }    ....}The same can be generated by IDEs such as Intelij, however, probably in an interview or a situation where you don’t have that luxury, it is important that you are on top of this.  Using an Utility LibraryAlso, common utility libraries such as Apache Commons provides builders to implement it easily.ex: Using Apache Commons (which uses reflections to perform equal check using attributes of the object. This could be the default implementation of a base class (BaseDto, BaseModel or BaseEntity)) public boolean equals(Object obj) {   return EqualsBuilder.reflectionEquals(this, obj); }  Using Lombok to generate the codeNow, if you are using Java, but hate boilerplate, probably you are already a fan of lombok. It provides a nice annotations called @EqualsAndHashCode which can use the generate both equals and hashCode methods automatically.  If you use kotlin, use data classes that generates equals and hashCodeData classes are one of those syntax sugar conveniences that makes us fall in love with the Kotlin. What Lombok does with Java, Data classes does it automatically in Kotlin. It generates equals / hashCode() pair based on the attributes defined in the primary constructor.ex:data class User(val name: String, val age: Int)equals() and hashCode() for the above will be generated based on name and age attributes.Now lets talk about the contract between equals() and hashCode(). The moment we change the logic of the equality operation, we need to ensure that hashCode() method contract still stands.HashCode - Covering BasicsWhy is hashCode() method so important?As we all know, hashCode() is used in Java’s Collections that uses Hashing mechanism as a way to minimize the performance cost of operations such as add, get, remove and etc. Examples are HashTable, HashMap and HashSet.What is the contract between hashCode() and equals() ?If two objects are equal, their hashCode() value must be the same. This the rule of thumb.However, if hashCode() values of two objects are the same, it does not mean that those two objects must be equal. It could be, but does not have to be. It all depends on how the developers have implemented the hashCode() method.Now the Rule of Thumb prescribed above is so important. Because if you don’t abide to this rule, it could lead to inconsistent results specially when you use HashSet like implementations.Example below:/**   * Assume that passports are equal only if their number and name matches.  */ public class Passport {        private String number;   	private String name;   	private String country;     	public Passport(String number, String name) {          this.number = number;   		this.name = name;   	}        @Override   	public final boolean equals(Object o) {          // Shallow Check   		if (this == o) return true;   		// if not from same class, not equal.   		if (o == null || !o.getClass().equals(Passport.class)) return false;   		Passport passport = (Passport) o;   		// NOTE: Null safe equality check using Objects.equal().   		// If any of the value is null, it will return false, // if both are null, it will return true. return Objects.equals(number, passport.number) &amp;amp;&amp;amp; Objects.equals(name, passport.name);   }      	public static void demoImportanceOfHashMap(){          Passport p1 = new Passport(&quot;123&quot;, &quot;Chinthaka&quot;);   		Passport p2 = new Passport(&quot;123&quot;, &quot;Chinthaka&quot;);   		// Prove that they are equal   		System.out.println(p1.equals(p2)); // returns true.     		// Lets add them to a HashSet Set&amp;lt;Passport&amp;gt; set = new HashSet&amp;lt;&amp;gt;();   		set.add(p1);   		set.add(p2);     		System.out.println(&quot;P1 HashCode:&quot; + p1.hashCode());   		System.out.println(&quot;P2 HashCode:&quot; + p2.hashCode());     		// Assumption is the size should be 1   		System.out.println(&quot;Set Size: &quot; + set.size());   	}     	public static void main(String[] args) {          demoImportanceOfHashMap();   	}  }The output was:trueP1 HashCode:356573597P2 HashCode:1735600054Set Size: 2As you see, even if two objects are equal, but their hash codes are not the same, they will not be considered equal in an HashSet (because hashCode() is invoked first and based on that only, an equals() operation will be executed IF there are multiple elements in a particular hash bucket) and therefore you would see duplicates. This is catastrophic.Another code smell of a hashCode() is when there are too much hash collisions. As written in HashMap Java Documentation, having too many different Objects (unequal) returning the same hash code is a sure way for reducing performance as it increases the number of elements of a particular Hash Bucket where the object is stored.Refer the below snippet from Java HashMap.getNode(int hash, Object key)  implementation.do {      if (e.hash == hash &amp;amp;&amp;amp;          ((k = e.key) == key || (key != null &amp;amp;&amp;amp; key.equals(k))))          return e;  } while ((e = e.next) != null);Java Object class provides a default implementation for the hashCode() method. How does Java implement hashCode?public native int hashCode();The native keyword here indicates that this is not a Java implementation, it is written using low level language (C) and then injected to Java using JNI (Java Native Interface) technique. So we will not be able to see how the implementation looks like here.The default implementation of the hashCode() is based on the memory address of the Object.  Reading through the native code (provided in the reference below), it further proves that it is uses different techniques such as MD5, CRC32, DES/AES and XOR Shifting to generate the hash code value based on the memory address. Knowing what really goes beneath our JVM, though it is tough to fully perceive, is certainly interesting. Refer to the links I have provided below.So how do we override and implement the hashCode() properly for a class which already has a custom equals() implementation.Codified Way - Every Java Developer Must KnowThis is based on the guidance provided by Joshua Bloch in Chapter 03 of Effective Java book.For the hash code to be distinctive as possible across different objects making it an effective hashCode() implementation, the following steps needs to be followed.  Store a non-zero value as the result. (Any random number)  Then for each field that is considered in the equals() implementation, calculate their hashcode and them to the multiplication of current result by 31.Refer an implementation of it below for the Passport class./**   *  * Instead of generating the hash code on based on the Object&#39;s Memory Reference (JNI),  * generate it based on the attributes that are involved in the equality check.  *  * @return  */  @Override public int hashCode() {      int result = 17;  	result = 31 * result + number == null? 0: number.hashCode();  	result = 31 * result + name == null? 0: name.hashCode();	return result;  }why a random number at the start? Zero cannot be used as it would lead to collisions. Specially when the hash code of the fields that are participating in this would also return 0 in their hash code calculation. For Example, boolean false value would generally have 0 as the hash code. Also if a particular field is null, typically 0 is returned (as depicted in the code). Having a non-zero number as the initial seed reduces the potential conflicts due to that reason.NOTE: However Boolean Java class does not return 0 (false) or 1 (true) for hashCode. Instead arbitrary numbers of 1231 and 1237 are used.Why multiply by number 31?The reason is that it makes the hashCode() calculation depend on the order of the fields. Great example from Effective Java book that I like to quote here is:“For example, if the multiplication were omitted from a String hash function, all anagrams would have identical hash codes.”I wondered why number 31 and the good reason that I found from that chapter is, multiplication by 31 can be replaced by a Shift and a Subtraction (31 * i == (i &amp;lt;&amp;lt; 5) - i) and modern VMs seems to do this optimization for us.Using Objects.hash(Object... values)This is an utility method provided in Java Objects library which takes cares of generating a hashCode following the above mentioned techniques applied with all the participating fields.Also, as mentioned above, Lombok for Java would auto-generate this for you with EqualsAndHashCode annotation. Similarly in Kotlin, Data Classes automatically generate this for you.As Java programmers, we no longer may really pay too much attention this. But the importance of equals() and hashCode() method combination remains the same, specially when you are building frameworks and components that other teams will rely on in the future.References      Apache Commons EqualsBuilder - https://commons.apache.org/proper/commons-lang/apidocs/org/apache/commons/lang3/builder/EqualsBuilder.html        Lombok - https://projectlombok.org/features/EqualsAndHashCode        Kotlin Data Classes - https://kotlinlang.org/docs/data-classes.html    Good Discussion on Java’s HashCode implementation          https://stackoverflow.com/questions/13602501/whats-the-implementation-of-hashcode-in-java-object            Found source code for Java Native Hash Code Implementation here - https://stackoverflow.com/questions/10578764/why-are-hashcode-and-getclass-native-methods    C Code for Java’s HashCode Implementation          http://hg.openjdk.java.net/jdk7/jdk7/hotspot/file/tip/src/share/vm/prims/jvm.cpp (JVM_ENTRY(jint, JVM_IHashCode…), line 504)      http://hg.openjdk.java.net/jdk7/jdk7/hotspot/file/tip/src/share/vm/runtime/synchronizer.cpp (FastHashCode, Line 576 and get_next_hash, line 530)      "
  },
  
  {
    "title": "How to Create a Helm Repo in Git?",
    "url": "/posts/how-to-create-git-based-helm-repo/",
    "categories": "",
    "tags": "",
    "date": "2019-01-08 00:00:00 +0000",
    





    "snippet": "If you know Helm, you know helm chart repositories. It is like maven repositories, instead these repositories hold onto thedifferent charts that can be used to install different tools and products into a kubernetes environment. If you are planning to use Helm in your organization to manage your own deployments, lets say your micro-services, then most probably you have come across the need for a chart repository of your own.According Helm:”A chart repository is an HTTP server that houses an index.yaml file and optionally some packaged charts. When you’re ready to share your charts, the preferred way to do so is by uploading them to a chart repository.“Source: https://github.com/helm/helm/blob/master/docs/chart_repository.mdSo all you simply need is a HTTP Server. The simplest of all the options that comes to mind is GIT in this case.If you organization is using GitHub, GitLab, BitBucket or any Git variation, you can easily create a helm repository.The following are the set of steps that I followed.Step 1: Your chart has to be packaged as a Tar File.$ helm package test-chartSuccessfully packaged chart and saved it to: /Users/admin/Development/Research/Helm/helm-repository/test-chart-0.0.1.tgzStep 2: Create a chart repository$ mkdir test-charts$ # Copy the charts and tar file$ mv test-chart test-chart-0.0.1.tgz ./test-chartsStep 3: Create a index.yaml file for the chart_repository$ cd test-charts$ helm repo index .Index.yaml file should look like the followingapiVersion: v1entries:  test-chart:  - apiVersion: v1    created: 2019-01-08T00:01:10.623604673-05:00    description: Simple Micro-service Chart    digest: 748594dc2815a12ea3a2b4860b731ff0f3d7cfce0102538214448dd962b8ac08    name: test-chart    urls:    - test-chart-0.0.1.tgz    version: 0.0.1generated: 2019-01-08T00:01:10.622439226-05:00As a pre-requisite, you need to have a Git Repo Ready.Mine is https://github.com/chinthakadd/simple-helm-repoStep 4: Initialize your repo and Push the chart repository to the remote Git repository$ git init$ git add .$ git commit -m &quot;init chart repo&quot;$ git remote add origin git@github.com:chinthakadd/simple-helm-repo$ git push -u origin masterStep 5: Add the repo to Helm$ helm repo add simple-helm-repo {Git Raw URL}NOTE: Git URL is the Raw Git URL.Ex: https://raw.githubusercontent.com/chinthakadd/simple-helm-repo/master for my chart repo.Step 6: Check if you can fetch a Helm Chart from the Repository$ helm fetch simple-helm-repo/test-chart --untarA simple asciicinema on how I achieved this."
  },
  
  {
    "title": "How to use JVisualVM and Java Mission Control to monitor Spring Boot Apps in Kubernetes?",
    "url": "/posts/JVisualVM-to-monitor-Spring-Boot-Apps-in-Kubernetes/",
    "categories": "",
    "tags": "",
    "date": "2018-11-15 00:00:00 +0000",
    





    "snippet": "I thought of writing this as it is useful starting point for any developer who wants to startmonitoring the JVMs of containers running as Kubernetes Pods.As I experienced during last few months, JVM optimization is one of the biggest challenges thatenterprises face in moving towards Cloud Platforms with Java based microservices.As often heard from many platform engineers, Java is “hungry” for Resources. It is true to a certainextent. Specifically if you have not sized your JVM properly using the boundary values for differentsegments of your JVM, it is definitely going to become a memory hungry beast and eventually killa node or two in your cluster.  So today, I am not intending to solve this entire problem. Theidea is to first get an idea of how a simple java - spring boot based application would consumein terms of memory.Though there are many elegant solutions for monitoring tools available out there, I am going tostart with the basics of tools that every java installation would come along with. Somethingthat works in your local. Something that you can start right away with to understand how yourteeny-tiny microservice is dealing with memory. It is called JVisualVM.One more tool that Java ships with is Java Mission Control (JMC),which has some more features available if you have the commercial license.These are standard monitoring tools that rely upon JMX Technology to connect with JVM based applications that expose their JVM metrics through a JMX endpoint.If you want to monitor an application that runs in yourlocalhost, it is extremely simple.  Run jvisualvm command to start jvisualvm or jmc commands to start java mission control.  Then start your application in localhost. Immediately you will see that your application is detected.Since java 6 SE, it automatically enables java agents running locally to monitor applications that arestarted locally. Therefore automatically you would see your java applications like Spring boot ones beingdisplayed in jvisualvm. See below.But our objective today is to monitor a Spring Boot Application deployed kubernetes.Java by default does not enable remove agents to connect to JVM’s JMX attach APIs, obviously forsecurity reasons.There are many properties that needs to enabled at the JVM to ensure the remote monitoring and managementcan be done. So now, assuming that you have a kubernetes deployment that contains a pod with yourJava based microservice, next step is to understand what are the JVM parameters that you needto enable to ensure the remote connections to JMX can be made.Lets first understand those JVM arguments.      -Dcom.sun.management.jmxremote (Enabling JMX Clients to access the JVM.)        -Dcom.sun.management.jmxremote.port=9999 (JMX Attach APIs will exposed through port 9999)        -Dcom.sun.management.jmxremote.rmi.port=9999 (It is required that we set RMI Port the same port.Reason will be more clearer when you realize what we do next. Essentially, we need a singleport from the POD container that can expose both JMX attach APIs as well as RMI)        -Dcom.sun.management.jmxremote.authenticate=false (We are setting this to false. Yes not agood security practice. But we are friends here. Only in our dev cluster right? :) )        -Dcom.sun.management.jmxremote.ssl=false (Again, for now disabling SSL.)        -Djava.rmi.server.hostname=127.0.0.1 (This is important. Again, you may wonderhow localhost IP would work here. Are we running minikube? No this will work evenfor remote clusters. Thats’ the funny part)  We will get to the question on server hostname being localhost in a bit.First thing, how do I easily add these to my pod deployment?Do I have to change my Dockerfile to include these arguments at the entry point? No.A simple trick is to offload these to the Kubernetes Deployment resource and injectthem to the container as environment variables. Simple right? Here is a sampleDeployment yaml.apiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: k8-jvisualvm-poc  namespace: defaultspec:  replicas: 1  selector:    matchLabels:      app: k8-jvisualvm-poc  template:    metadata:      labels:        app: k8-jvisualvm-poc    spec:      containers:      - name: k8-jvisualvm-poc        image: chinthaka316/k8-jvisualvm-poc:latest        ports:        - containerPort: 8080          name: http          protocol: TCP        imagePullPolicy: Always        env:          ## This is the environment property which we use to inject all the required JMX related system properties        - name: JAVA_TOOL_OPTIONS          value: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.rmi.port=9999 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=127.0.0.1Finally, connecting to the Remote HostNow we want to connect to the running pod. My container is running as following.$ kubectl get pods|grep k8-jvisualvm-pock8-jvisualvm-poc-b54569c89-k2nld   1/1       Running   0          4mLets use kubectl port-forward to tunnel the container port 9999 JMX endpointto localhost’s 9999 port.$ kubectl port-forward $(kubectl get pods -n default|grep k8-jvisualvm-poc|awk &#39;{print $1}&#39;) 9999:9999Forwarding from 127.0.0.1:9999 -&amp;gt; 9999Forwarding from [::1]:9999 -&amp;gt; 9999Now, simply we can add a new connection in JVisualVM as below.Voila.! You should see your application monitored as below.You can do the same with Java Mission Control as well. Simply add a new jmx connection with sameproperties as above.You can find all resources related to this blogpost in:https://github.com/chinthakadd/k8-jvisualvm-pocWhat’s Next?Some of the topics that I have been interested in looking into are follows.  Learn about JVM deeply to understand what are most important boundary parametersthat needs to be set.  Understand Kubernetes resource limit concept, Specially on CPU and memory  Learn about Spring Boot with MicroMeter and what metrics that we can expose  How to effectively use Prometheus and Grafana in combination to extractuseful metrics out of Spring Boot Applications and do pod level monitoringReferences:https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jvisualvm.htmlhttps://docs.oracle.com/javase/7/docs/technotes/guides/management/agent.htmlhttps://fedidat.com/250-jvisualvm-openshift-pod/http://darksideofthedev.com/java-profiling-with-visualvm-in-docker-container/http://www.adam-bien.com/roller/abien/entry/how_to_establish_jmx_connection"
  }
  
]

